{
  "_args": [
    [
      {
        "raw": "scrape-it",
        "scope": null,
        "escapedName": "scrape-it",
        "name": "scrape-it",
        "rawSpec": "",
        "spec": "latest",
        "type": "tag"
      },
      "C:\\Users\\Jacob2\\source\\repos\\dinoBot\\dinoBot"
    ]
  ],
  "_from": "scrape-it@latest",
  "_id": "scrape-it@5.0.5",
  "_inCache": true,
  "_location": "/scrape-it",
  "_nodeVersion": "8.11.1",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/scrape-it_5.0.5_1526032753382_0.16327505937058118"
  },
  "_npmUser": {
    "name": "ionicabizau",
    "email": "bizauionica@gmail.com"
  },
  "_npmVersion": "5.6.0",
  "_phantomChildren": {
    "core-util-is": "1.0.2",
    "css-select": "1.2.0",
    "dom-serializer": "0.1.0",
    "domelementtype": "1.3.0",
    "domutils": "1.5.1",
    "entities": "1.1.1",
    "inherits": "2.0.3",
    "jsdom": "7.2.2",
    "lodash": "4.17.10"
  },
  "_requested": {
    "raw": "scrape-it",
    "scope": null,
    "escapedName": "scrape-it",
    "name": "scrape-it",
    "rawSpec": "",
    "spec": "latest",
    "type": "tag"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/scrape-it/-/scrape-it-5.0.5.tgz",
  "_shasum": "0ae9282243df012163d6f4c367d34ef100473996",
  "_shrinkwrap": null,
  "_spec": "scrape-it",
  "_where": "C:\\Users\\Jacob2\\source\\repos\\dinoBot\\dinoBot",
  "author": {
    "name": "Ionică Bizău",
    "email": "bizauionica@gmail.com",
    "url": "https://ionicabizau.net"
  },
  "blah": {
    "h_img": "https://i.imgur.com/j3Z0rbN.png",
    "cli": "scrape-it-cli",
    "installation": [
      {
        "h2": "FAQ"
      },
      {
        "p": "Here are some frequent questions and their answers."
      },
      {
        "h3": "1. How to parse scrape pages?"
      },
      {
        "p": "`scrape-it` has only a simple request module for making requests. That means you cannot directly parse ajax pages with it, but in general you will have those scenarios:"
      },
      {
        "ol": [
          "**The ajax response is in JSON format.** In this case, you can make the request directly, without needing a scraping library.",
          "**The ajax response gives you HTML back.** Instead of calling the main website (e.g. example.com), pass to `scrape-it` the ajax url (e.g. `example.com/api/that-endpoint`) and you will you will be able to parse the response",
          "**The ajax request is so complicated that you don't want to reverse-engineer it.** In this case, use a headless browser (e.g. Google Chrome, Electron, PhantomJS) to load the content and then use the `.scrapeHTML` method from scrape it once you get the HTML loaded on the page."
        ]
      },
      {
        "h3": "2. Crawling"
      },
      {
        "p": "There is no fancy way to crawl pages with `scrape-it`. For simple scenarios, you can parse the list of urls from the initial page and then, using Promises, parse each page. Also, you can use a different crawler to download the website and then use the `.scrapeHTML` method to scrape the local files."
      },
      {
        "h3": "3. Local files"
      },
      {
        "p": "Use the `.scrapeHTML` to parse the HTML read from the local files using `fs.readFile`."
      }
    ]
  },
  "bugs": {
    "url": "https://github.com/IonicaBizau/scrape-it/issues"
  },
  "contributors": [
    {
      "name": "ComFreek",
      "email": "comfreek@outlook.com",
      "url": "https://github.com/ComFreek"
    }
  ],
  "dependencies": {
    "assured": "^1.0.10",
    "cheerio": "^0.20.0",
    "cheerio-req": "^1.2.0",
    "err": "^2.1.8",
    "is-empty-obj": "^1.0.9",
    "iterate-object": "^1.3.2",
    "obj-def": "^1.0.6",
    "typpy": "^2.3.9"
  },
  "description": "A Node.js scraper for humans.",
  "devDependencies": {
    "@types/cheerio": "^0.22.6",
    "lien": "^1.0.1",
    "tester": "^1.4.1"
  },
  "directories": {},
  "dist": {
    "integrity": "sha512-H+qXFE2xN3NQtSc1p6sQCt0EFahrQwhVtlcvxkXCf5y0Pn7AeE5XCrHo6mTuQmWN+cge6LlD8rV+UcoAQRvq7A==",
    "shasum": "0ae9282243df012163d6f4c367d34ef100473996",
    "tarball": "https://registry.npmjs.org/scrape-it/-/scrape-it-5.0.5.tgz",
    "fileCount": 5,
    "unpackedSize": 28075,
    "npm-signature": "-----BEGIN PGP SIGNATURE-----\r\nVersion: OpenPGP.js v3.0.4\r\nComment: https://openpgpjs.org\r\n\r\nwsFcBAEBCAAQBQJa9WlyCRA9TVsSAnZWagAAWcAP/Rq6nndgphVPRFpiEatK\nTxK7K/0fR9BhNn8TfpSzG9sdCsJwxhrv4Xsa+Z5K5+4pXzocHXj34kf0Rx8R\nfmDJlJLbSA7xvHOKvynxgOr2TD+nyMOk8gplbIpCEPpsjDrbi8OheFL1HaUB\nLfYcaPbHKQLKNfBU77hNeTK7okj2ZRGAFngl36o0uiXlT3h2ydhl8CKSYggo\nOmzeeSqEhR7q+hPQDvt65LnTbVXg6bjTiTpkG01Tb7YHT8oMhYyKP6CmtDdK\nAWXettK2ZVvxhy3yH36Jq+MlNU77wo3ZHn5GiigIHvPjQn4wnVS8EsZCduDw\n3u0lFRLPPPA3zDDdAzZOGV0eVqvKnK7HJDaDg1RwP9diISfkinLIKsj9cgpD\n6Qaw6oQ03O7fIjqmAsYpQl2R0KTPf/ZL7J6bfQmKzeOR7bB2U7z//GreujUg\nM0qrQvSkgNTenqfluC/HM/6PvSpiO98vKnwPKQogtDLwOMnX7goxQDRu7r0R\n1Pr2F3/khso9zio8mi6Y2+Lv+XCpqoS0R4f/Fv3ax6AxWHxeqga+KvLA+Q1X\nWrmTpS0m2zGvumHAiqnTLiWdQRZrq5sICGipZDsFskqeNblwU3nozwrl/At5\nefDJ4/fKXxZoE6+gc+sFNlwTtFdBjASpa9DiBAZVGY7mwwCTwgiKnkt75FZ2\nE8k4\r\n=fX0W\r\n-----END PGP SIGNATURE-----\r\n"
  },
  "files": [
    "bin/",
    "app/",
    "lib/",
    "dist/",
    "src/",
    "scripts/",
    "resources/",
    "menu/",
    "cli.js",
    "index.js",
    "bloggify.js",
    "bloggify.json",
    "bloggify/"
  ],
  "gitHead": "d6ac40ace97972df3d4a592a876dc5fd9f7788f4",
  "homepage": "https://github.com/IonicaBizau/scrape-it#readme",
  "keywords": [
    "scrape",
    "it",
    "a",
    "scraping",
    "module",
    "for",
    "humans"
  ],
  "license": "MIT",
  "main": "lib/index.js",
  "maintainers": [
    {
      "name": "ionicabizau",
      "email": "bizauionica@gmail.com"
    }
  ],
  "name": "scrape-it",
  "optionalDependencies": {},
  "readme": "ERROR: No README data found!",
  "repository": {
    "type": "git",
    "url": "git+ssh://git@github.com/IonicaBizau/scrape-it.git"
  },
  "scripts": {
    "test": "node test"
  },
  "types": "lib/index.d.ts",
  "version": "5.0.5"
}
